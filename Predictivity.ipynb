{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNKbxSA0fNiPYx2oNSKajFG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rauan2308/Empirically-assessing-the-predicitivty-of-tourism-demand-data/blob/main/Predictivity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXfY0own9SRE"
      },
      "outputs": [],
      "source": [
        "\"Preparing data for the experimental part\"\n",
        "# Download the excel file from ABS: Table 11: Short-term movement, visitors arriving - state of intended stay: original\n",
        "# In the first row keep only the destination names: Total, NSW, Vic, Qld, SA, WA, Tas, NT, ACT\n",
        "# Delete the rows: Unit, Series Type, .. up to Series ID\n",
        "# Add COVID column with dummy variables: 0 - from Jan 1991 to Jan 2020, 1 - from Feb 2020 to Oct 2024\n",
        "# Upload the file using the code below\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "!pip install openpyxl  # Install openpyxl if needed\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import io\n",
        "file_name = list(uploaded.keys())[0]  # Get the uploaded file name\n",
        "\n",
        "# Read Excel file\n",
        "df = pd.read_excel(io.BytesIO(uploaded[file_name]), engine='openpyxl')\n",
        "\n",
        "# Ensure the first column is treated as dates\n",
        "df.iloc[:, 0] = pd.to_datetime(df.iloc[:, 0], errors='coerce')\n",
        "\n",
        "# Set the first column as the index\n",
        "df.set_index(df.columns[0], inplace=True)\n",
        "\n",
        "# Display the first few rows\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"Calculation of Sample Entropy and MultiSampEn\"\n",
        "\n",
        "!pip install pyentrp\n",
        "from scipy.optimize import fsolve\n",
        "from cmath import log\n",
        "from pyentrp.entropy import multiscale_entropy\n",
        "\n",
        "# Function to calculate phi_max for each entropy value\n",
        "def phi_max(s_values, length): #\n",
        "    phi_values = []\n",
        "    for s in s_values:\n",
        "        func = lambda x: (-(x * log(x, 2).real + (1 - x) * log(1 - x, 2).real) + (1 - x) * log(length - 1, 2).real) - s\n",
        "        ub = fsolve(func, 0.1)[0]  # Solve for upper bound\n",
        "        phi_values.append(ub)\n",
        "    return phi_values  # Return phi_max for each sub-series\n",
        "\n",
        "# Sample Entropy Calculation\n",
        "def entropy_cal(input_data, length, order):\n",
        "    X_10 = []\n",
        "    S10_s1 = []\n",
        "    n = length\n",
        "\n",
        "    # Create sub-series\n",
        "    for index in range(len(input_data) - n + 1):\n",
        "        X_10.append(input_data[index: index + n])\n",
        "\n",
        "    # Calculate Sample Entropy (SampEn) for each sub-series\n",
        "    for sub_series in X_10:\n",
        "        sampen = ent.multiscale_entropy(sub_series, order, 0.65 * np.std(sub_series), 6)\n",
        "        sampen = [val / np.log(2) for val in sampen]  # Convert to log base 2\n",
        "        S10_s1.append(sampen[0])  # Store only the first scale SampEn value, change to store a required scale\n",
        "\n",
        "    return S10_s1  # Return all entropy values for sub-series"
      ],
      "metadata": {
        "id": "2raX07PuGCLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"Calculation of WPE and Predictivity(WPE)\"\n",
        "\n",
        "from pyentrp.entropy import weighted_permutation_entropy\n",
        "\n",
        "# Function to extract subseries\n",
        "def subseries_cal(input_series, length):\n",
        "    sub_series_list = []\n",
        "    for index in range(len(input_series) - length + 1):\n",
        "        sub_series = input_series[index: index + length]\n",
        "        sub_series_list.append(sub_series)\n",
        "    return sub_series_list\n",
        "\n",
        "# WPE Calculation\n",
        "def pred_wpe(input_series, order, delay):\n",
        "    return weighted_permutation_entropy(input_series, order, delay, normalize=False)\n",
        "\n",
        "# Predictivity(WPE) Calculation\n",
        "def pred_wpe(input_series, order, delay):\n",
        "    return 1 - weighted_permutation_entropy(input_series, order, delay, normalize=True)"
      ],
      "metadata": {
        "id": "B4mQ0_d4jhen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"Calculation of RMSPE\"\n",
        "\n",
        "def rmspe(y_true, y_pred):\n",
        "    \"\"\"Calculate the root mean squared percentage error (RMSPE).\"\"\"\n",
        "    if np.any(y_true == 0):\n",
        "        return np.nan  # Or a large value, or handle this appropriately\n",
        "    if len(y_true) == 0 or len(y_pred) == 0:  # Handle empty input arrays\n",
        "        return np.nan\n",
        "    if len(y_true) != len(y_pred):\n",
        "        print(\"Error: y_true and y_pred have different lengths in RMSPE calculation.\")\n",
        "        return np.nan\n",
        "    percentage_errors = np.abs((y_true - y_pred) / y_true)\n",
        "    mse = np.mean(percentage_errors**2)\n",
        "    return np.sqrt(mse)"
      ],
      "metadata": {
        "id": "vMgUuThAjUSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"Seasonal-Trend decomposition using STL\"\n",
        "\n",
        "!pip install pyentrp pyts\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "\n",
        "def sstl_decomposition(series):\n",
        "    series = pd.Series(series)\n",
        "    stl = seasonal_decompose(series, model='additive', period=24, extrapolate_trend='freq')\n",
        "    stl_trend_seasonal = stl.trend.fillna(method=\"bfill\").fillna(method=\"ffill\")+stl.seasonal\n",
        "    return stl_trend_seasonal"
      ],
      "metadata": {
        "id": "DgM_b8xQkXEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"Singular Spectrum Analysis (SSA) decomposition and sum the first `num_components` components.\"\n",
        "\n",
        "from pyts.decomposition import SingularSpectrumAnalysis as SSA\n",
        "\n",
        "def ssa_decomposition(series, num_components=2):\n",
        "    series = np.array(series).reshape(1, -1)\n",
        "    ssa = SSA(window_size=12)\n",
        "    X_s = ssa.fit_transform(series)\n",
        "    num_components = min(num_components, X_s.shape[1])\n",
        "    reconstructed = np.sum(X_s[:, :num_components], axis=1)\n",
        "    return reconstructed.flatten()"
      ],
      "metadata": {
        "id": "otxbYNGulH0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"ARIMA model\"\n",
        "\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "\n",
        "# Function to find the best ARIMA order\n",
        "def find_best_arima(series, p_range=(0, 3), d_range=(0, 3), q_range=(0, 3)):\n",
        "    best_aic = float(\"inf\")\n",
        "    best_order = None\n",
        "    for p, d, q in product(range(*p_range), range(*d_range), range(*q_range)):\n",
        "        try:\n",
        "            model = ARIMA(series, order=(p, d, q))\n",
        "            model_fit = model.fit()\n",
        "            if model_fit.aic < best_aic:\n",
        "                best_aic = model_fit.aic\n",
        "                best_order = (p, d, q)\n",
        "        except:\n",
        "            continue\n",
        "    return best_order\n",
        "\n",
        "# Function to forecast using ARIMA\n",
        "def arima_forecast(series, forecast_horizon=12):\n",
        "    series = np.array(series)\n",
        "    train, test = series[:-forecast_horizon], series[-forecast_horizon:]\n",
        "    best_order = find_best_arima(train)\n",
        "    if best_order is None:\n",
        "        print(\"Warning: No suitable ARIMA model found.\")\n",
        "        return np.full(forecast_horizon, np.nan), test\n",
        "    model = ARIMA(train, order=best_order)\n",
        "    model_fit = model.fit()\n",
        "    forecast = model_fit.forecast(steps=forecast_horizon)\n",
        "    return forecast, test"
      ],
      "metadata": {
        "id": "nycvBCpolxK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"Kendall's and Spearman's correlations\"\n",
        "\n",
        "from scipy.stats import kendalltau, spearmanr\n",
        "\n",
        "# Function to compute correlation scores\n",
        "def compute_correlations(data):\n",
        "    kendall_rmspe_predictivity, kendall_p_predictivity = stats.kendalltau(data[\"RMSPE\"], data[\"Predictivity\"])\n",
        "    spearman_rmspe_predictivity, spearman_p_predictivity = stats.spearmanr(data[\"RMSPE\"], data[\"Predictivity\"])\n",
        "\n",
        "    kendall_rmspe_phi, kendall_p_phi = stats.kendalltau(data[\"RMSPE\"], data[\"Phi_max\"])\n",
        "    spearman_rmspe_phi, spearman_p_phi = stats.spearmanr(data[\"RMSPE\"], data[\"Phi_max\"])\n",
        "\n",
        "    results = {\n",
        "        \"Kendall's Tau (RMSPE vs Predictivity)\": kendall_rmspe_predictivity,\n",
        "        \"Kendall's p-value (RMSPE vs Predictivity)\": kendall_p_predictivity,\n",
        "        \"Spearman's Rho (RMSPE vs Predictivity)\": spearman_rmspe_predictivity,\n",
        "        \"Spearman's p-value (RMSPE vs Predictivity)\": spearman_p_predictivity,\n",
        "        \"Kendall's Tau (RMSPE vs Phi_max)\": kendall_rmspe_phi,\n",
        "        \"Kendall's p-value (RMSPE vs Phi_max)\": kendall_p_phi,\n",
        "        \"Spearman's Rho (RMSPE vs Phi_max)\": spearman_rmspe_phi,\n",
        "        \"Spearman's p-value (RMSPE vs Phi_max)\": spearman_p_phi\n",
        "    }\n",
        "\n",
        "    return pd.DataFrame([results])"
      ],
      "metadata": {
        "id": "JttWjUYosdPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"WPE calculation for tuning\"\n",
        "\n",
        "!pip install pyentrp\n",
        "# prepare the library on entropy calculation\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas import read_csv\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import FuncFormatter\n",
        "from matplotlib.dates import YearLocator, DateFormatter\n",
        "from pyentrp import entropy as ent\n",
        "import seaborn as sns\n",
        "from scipy.optimize import fsolve\n",
        "from cmath import log\n",
        "from pylab import rcParams\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from pyentrp.entropy import weighted_permutation_entropy\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from scipy.optimize import fsolve\n",
        "from math import log\n",
        "\n",
        "X=df.Total.values.reshape(406,)\n",
        "\n",
        "def calculate_wpe_for_combinations(X, orders, delays):\n",
        "    results = []\n",
        "\n",
        "    # Iterate over all combinations of order and delay\n",
        "    for order in orders:\n",
        "        for delay in delays:\n",
        "            # Calculate WPE for the current order and delay\n",
        "            wpe = weighted_permutation_entropy(X, order, delay, normalize=False)\n",
        "            results.append((order, delay, wpe))\n",
        "\n",
        "    return results\n",
        "\n",
        "orders = range(3,5)\n",
        "delays = range(1,21)\n",
        "\n",
        "# Calculate WPE\n",
        "phi_results = calculate_wpe_for_combinations(X, orders, delays)\n",
        "\n",
        "# Convert results to a DataFrame for easy plotting\n",
        "phi_results = pd.DataFrame(phi_results, columns=['Order', 'Delay', 'WPE'])\n",
        "\n",
        "# Create the line plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "for order in orders:\n",
        "    subset = phi_results[phi_results['Order'] == order]\n",
        "    plt.plot(subset['Delay'], subset['WPE'], label=f'm = {order}')\n",
        "\n",
        "plt.xticks(ticks=delays)\n",
        "plt.grid(False)\n",
        "\n",
        "plt.xlabel(f'Time delay, $\\\\tau$')\n",
        "plt.ylabel('WPE')\n",
        "plt.title('WPE for embedding dimension $m=3,4$ and time delay $\\\\tau = 1-20$')\n",
        "plt.legend(title='Embedding dimension', loc='upper center', bbox_to_anchor=(0.5, -0.13), ncol=3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "befDn-PxDzz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"Predictivity(WPE) calculation for tuning\"\n",
        "\n",
        "def calculate_wpe_for_combinations(X, orders, delays):\n",
        "    results = []\n",
        "\n",
        "    # Iterate over all combinations of order and delay\n",
        "    for order in orders:\n",
        "        for delay in delays:\n",
        "            # Calculate WPE for the current order and delay\n",
        "            phi = 1 - weighted_permutation_entropy(X, order, delay, normalize=True)\n",
        "            results.append((order, delay, phi))\n",
        "\n",
        "    return results\n",
        "\n",
        "orders = range(3,5)\n",
        "delays = range(1,21)\n",
        "\n",
        "# Calculate WPE\n",
        "wpe_results = calculate_wpe_for_combinations(X, orders, delays)\n",
        "\n",
        "# Convert results to a DataFrame for easy plotting\n",
        "df_ = pd.DataFrame(wpe_results, columns=['Order', 'Delay', 'WPE'])\n",
        "\n",
        "# Create the line plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "for order in orders:\n",
        "    subset = df_[df_['Order'] == order]\n",
        "    plt.plot(subset['Delay'], subset['WPE'], label=f'm = {order}')\n",
        "\n",
        "plt.xticks(ticks=delays)\n",
        "plt.grid(False)\n",
        "\n",
        "plt.xlabel(f'Time delay, $\\\\tau$')\n",
        "plt.ylabel('Predictivity')\n",
        "plt.title('Predictivity for embedding dimension $m=3,4$ and time delay $\\\\tau = 1-20$')\n",
        "plt.legend(title='Embedding dimension', loc='upper center', bbox_to_anchor=(0.5, -0.13), ncol=3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "j2zNYb8oHcO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"WPE calculation by varying data length and scale\"\n",
        "\n",
        "#!pip install pyentrp\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from pyentrp.entropy import weighted_permutation_entropy\n",
        "\n",
        "#!pip install pyentrp\n",
        "\n",
        "X = df.Total.values.reshape(406,)\n",
        "\n",
        "# Define different resolutions\n",
        "time_scales = {\n",
        "    \"monthly\": (X, 12, 349),  # Original monthly data\n",
        "    \"quarterly\": (X[:(406 // 3) * 3].reshape(-1, 3).mean(axis=1), 4, 116),  # Averaged quarterly data\n",
        "    \"half-yearly\": (X[:(406 // 6) * 6].reshape(-1, 6).mean(axis=1), 2, 58)  # Averaged half-yearly data\n",
        "    #\"yearly\": (X[:(406 // 12) * 12].reshape(-1, 12).mean(axis=1), 1, 29)  # Averaged yearly data\n",
        "}\n",
        "\n",
        "# Storage for results\n",
        "results_f = {}\n",
        "results_pre = {}\n",
        "results_post = {}\n",
        "\n",
        "# Function to extract subseries and COVID flags\n",
        "def subseries_cal(input_series, length, post_covid_index):\n",
        "    sub_series_list = []\n",
        "    post_covid_flags = []\n",
        "\n",
        "    for index in range(len(input_series) - length + 1):\n",
        "        sub_series = input_series[index: index + length]\n",
        "        sub_series_list.append(sub_series)\n",
        "\n",
        "        # Determine pre- or post-COVID based on index\n",
        "        post_covid_flags.append(1 if index + length - 1 >= post_covid_index else 0)\n",
        "\n",
        "    return sub_series_list, post_covid_flags\n",
        "\n",
        "# Function to calculate WPE\n",
        "def entropy_cal(input_series, order, delay):\n",
        "    wpe_values = [\n",
        "        weighted_permutation_entropy(input_series[i: i + len(input_series)], order, delay, normalize=False)\n",
        "        for i in range(len(input_series) - len(input_series) + 1)\n",
        "    ]\n",
        "    return np.mean(wpe_values) if wpe_values else 0\n",
        "\n",
        "# Loop over time scales\n",
        "for scale, (X_scaled, conversion_factor, post_covid_index) in time_scales.items():\n",
        "    m_f_dict = {}\n",
        "    m_pre_dict = {}\n",
        "    m_post_dict = {}\n",
        "\n",
        "    # Define appropriate length ranges\n",
        "    min_length, max_length, step = {\n",
        "        \"monthly\": (60, 406, 12),\n",
        "        \"quarterly\": (32, 136, 4),\n",
        "        \"half-yearly\": (32, 68, 2)\n",
        "        #\"yearly\": (20, 34, 1)\n",
        "    }[scale]\n",
        "\n",
        "    lengths = range(min_length, max_length, step)\n",
        "    orders = range(3, 4)\n",
        "    delays = range(step, step + 1)\n",
        "\n",
        "    # Calculate WPE for each length\n",
        "    for length in lengths:\n",
        "        sub_series_list, post_covid_flags = subseries_cal(X_scaled, length, post_covid_index)\n",
        "\n",
        "        wpe_values = []\n",
        "        wpe_pre = []\n",
        "        wpe_post = []\n",
        "\n",
        "        for i, sub_series in enumerate(sub_series_list):\n",
        "            for order in orders:\n",
        "                for delay in delays:\n",
        "                    avg_wpe = entropy_cal(sub_series, order, delay)\n",
        "                    wpe_values.append(avg_wpe)\n",
        "\n",
        "                    if post_covid_flags[i] == 1:\n",
        "                        wpe_post.append(avg_wpe)\n",
        "                    else:\n",
        "                        wpe_pre.append(avg_wpe)\n",
        "\n",
        "        # Convert length to years\n",
        "        length_in_years = length / conversion_factor\n",
        "\n",
        "        # Store averages per length\n",
        "        if wpe_values:\n",
        "            m_f_dict[length_in_years] = np.mean(wpe_values)\n",
        "        if wpe_pre:\n",
        "            m_pre_dict[length_in_years] = np.mean(wpe_pre)\n",
        "        if wpe_post:\n",
        "            m_post_dict[length_in_years] = np.mean(wpe_post)\n",
        "\n",
        "    # Store results for each time scale\n",
        "    results_f[scale] = list(m_f_dict.items())\n",
        "    results_pre[scale] = list(m_pre_dict.items())\n",
        "    results_post[scale] = list(m_post_dict.items())\n",
        "\n",
        "# Prepare the plot for full, pre-COVID, and post-COVID data\n",
        "fig, axs = plt.subplots(2, 1, figsize=(10, 12))\n",
        "\n",
        "# Plot full data\n",
        "for scale, data in results_f.items():\n",
        "    x_vals, y_vals = zip(*data)\n",
        "    axs[0].plot(x_vals, y_vals, label=f'{scale} scale')\n",
        "\n",
        "axs[0].set_title('Full time series')\n",
        "axs[0].set_xlabel('Data Length (Years)')\n",
        "axs[0].set_ylabel('WPE')\n",
        "axs[0].legend(loc='best')\n",
        "axs[0].grid(False)\n",
        "\n",
        "# Set x-ticks with step 1 for full data, using np.arange for float values\n",
        "x_vals_full = [x for scale, data in results_f.items() for x, _ in data]\n",
        "axs[0].set_xticks(np.arange(min(x_vals_full), max(x_vals_full) + 1, 1))\n",
        "axs[0].set_ylim(0.4, 2.1)  # Adjust y-axis limits (adjust as needed)\n",
        "\n",
        "# Plot pre-COVID data\n",
        "for scale, data in results_pre.items():\n",
        "    x_vals, y_vals = zip(*data)\n",
        "    axs[1].plot(x_vals, y_vals, label=f'{scale} time scale')\n",
        "\n",
        "axs[1].set_title('Pre-COVID time series')\n",
        "axs[1].set_xlabel('Data Length (Years)')\n",
        "axs[1].set_ylabel('WPE')\n",
        "#axs[1].legend(loc='best')\n",
        "axs[1].grid(False)\n",
        "\n",
        "# Set x-ticks with step 1 for pre-COVID data, using np.arange for float values\n",
        "x_vals_pre = [x for scale, data in results_pre.items() for x, _ in data]\n",
        "axs[1].set_xticks(np.arange(min(x_vals_pre), max(x_vals_pre) + 1, 1))\n",
        "axs[1].set_ylim(0.4, 2.1)  # Adjust y-axis limits (adjust as needed)"
      ],
      "metadata": {
        "id": "aCOAww-7H-_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"Predictivity(WPE) calculation by varying data length and scale\"\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from pyentrp.entropy import weighted_permutation_entropy\n",
        "\n",
        "X = df.Total.values.reshape(406,)\n",
        "\n",
        "# Define different resolutions\n",
        "time_scales = {\n",
        "    \"monthly\": (X, 12, 349),  # Original monthly data\n",
        "    \"quarterly\": (X[:(406 // 3) * 3].reshape(-1, 3).mean(axis=1), 4, 116),  # Averaged quarterly data\n",
        "    \"half-yearly\": (X[:(406 // 6) * 6].reshape(-1, 6).mean(axis=1), 2, 58)  # Averaged half-yearly data\n",
        "    #\"yearly\": (X[:(406 // 12) * 12].reshape(-1, 12).mean(axis=1), 1, 29)  # Averaged yearly data\n",
        "}\n",
        "\n",
        "# Storage for results\n",
        "results_f = {}\n",
        "results_pre = {}\n",
        "results_post = {}\n",
        "\n",
        "# Function to extract subseries and COVID flags\n",
        "def subseries_cal(input_series, length, post_covid_index):\n",
        "    sub_series_list = []\n",
        "    post_covid_flags = []\n",
        "\n",
        "    for index in range(len(input_series) - length + 1):\n",
        "        sub_series = input_series[index: index + length]\n",
        "        sub_series_list.append(sub_series)\n",
        "\n",
        "        # Determine pre- or post-COVID based on index\n",
        "        post_covid_flags.append(1 if index + length - 1 >= post_covid_index else 0)\n",
        "\n",
        "    return sub_series_list, post_covid_flags\n",
        "\n",
        "# Function to calculate WPE\n",
        "def entropy_cal(input_series, order, delay):\n",
        "    wpe_values = [\n",
        "        1-weighted_permutation_entropy(input_series[i: i + len(input_series)], order, delay, normalize=True)\n",
        "        for i in range(len(input_series) - len(input_series) + 1)\n",
        "    ]\n",
        "    return np.mean(wpe_values) if wpe_values else 0\n",
        "\n",
        "# Loop over time scales\n",
        "for scale, (X_scaled, conversion_factor, post_covid_index) in time_scales.items():\n",
        "    m_f_dict = {}\n",
        "    m_pre_dict = {}\n",
        "    m_post_dict = {}\n",
        "\n",
        "    # Define appropriate length ranges\n",
        "    min_length, max_length, step = {\n",
        "        \"monthly\": (60, 406, 12),\n",
        "        \"quarterly\": (32, 136, 4),\n",
        "        \"half-yearly\": (32, 68, 2)\n",
        "        #\"yearly\": (20, 34, 1)\n",
        "    }[scale]\n",
        "\n",
        "    lengths = range(min_length, max_length, step)\n",
        "    orders = range(3, 4)\n",
        "    delays = range(step, step + 1)\n",
        "\n",
        "    # Calculate WPE for each length\n",
        "    for length in lengths:\n",
        "        sub_series_list, post_covid_flags = subseries_cal(X_scaled, length, post_covid_index)\n",
        "\n",
        "        wpe_values = []\n",
        "        wpe_pre = []\n",
        "        wpe_post = []\n",
        "\n",
        "        for i, sub_series in enumerate(sub_series_list):\n",
        "            for order in orders:\n",
        "                for delay in delays:\n",
        "                    avg_wpe = entropy_cal(sub_series, order, delay)\n",
        "                    wpe_values.append(avg_wpe)\n",
        "\n",
        "                    if post_covid_flags[i] == 1:\n",
        "                        wpe_post.append(avg_wpe)\n",
        "                    else:\n",
        "                        wpe_pre.append(avg_wpe)\n",
        "\n",
        "        # Convert length to years\n",
        "        length_in_years = length / conversion_factor\n",
        "\n",
        "        # Store averages per length\n",
        "        if wpe_values:\n",
        "            m_f_dict[length_in_years] = np.mean(wpe_values)\n",
        "        if wpe_pre:\n",
        "            m_pre_dict[length_in_years] = np.mean(wpe_pre)\n",
        "        if wpe_post:\n",
        "            m_post_dict[length_in_years] = np.mean(wpe_post)\n",
        "\n",
        "    # Store results for each time scale\n",
        "    results_f[scale] = list(m_f_dict.items())\n",
        "    results_pre[scale] = list(m_pre_dict.items())\n",
        "    results_post[scale] = list(m_post_dict.items())\n",
        "\n",
        "# Prepare the plot for full, pre-COVID, and post-COVID data\n",
        "fig, axs = plt.subplots(2, 1, figsize=(10, 12))\n",
        "\n",
        "# Plot full data\n",
        "for scale, data in results_f.items():\n",
        "    x_vals, y_vals = zip(*data)\n",
        "    axs[0].plot(x_vals, y_vals, label=f'{scale} time scale')\n",
        "\n",
        "axs[0].set_title('Full time series')\n",
        "axs[0].set_xlabel('Data Length (Years)')\n",
        "axs[0].set_ylabel('Predictivity')\n",
        "#axs[0].legend(loc='best')\n",
        "axs[0].grid(False)\n",
        "\n",
        "# Set x-ticks with step 1 for full data, using np.arange for float values\n",
        "x_vals_full = [x for scale, data in results_f.items() for x, _ in data]\n",
        "axs[0].set_xticks(np.arange(min(x_vals_full), max(x_vals_full) + 1, 1))\n",
        "axs[0].set_ylim(0, 1)  # Adjust y-axis limits (adjust as needed)\n",
        "\n",
        "\n",
        "# Plot pre-COVID data\n",
        "for scale, data in results_pre.items():\n",
        "    x_vals, y_vals = zip(*data)\n",
        "    axs[1].plot(x_vals, y_vals, label=f'{scale} time series')\n",
        "\n",
        "axs[1].set_title('Pre-COVID time series')\n",
        "axs[1].set_xlabel('Data Length (Years)')\n",
        "axs[1].set_ylabel('Predictivity')\n",
        "#axs[1].legend(loc='best')\n",
        "axs[1].grid(False)\n",
        "\n",
        "# Set x-ticks with step 1 for pre-COVID data, using np.arange for float values\n",
        "x_vals_pre = [x for scale, data in results_pre.items() for x, _ in data]\n",
        "axs[1].set_xticks(np.arange(min(x_vals_pre), max(x_vals_pre) + 1, 1))\n",
        "axs[1].set_ylim(0, 1)  # Adjust y-axis limits (adjust as needed)\n"
      ],
      "metadata": {
        "id": "Ifo7PEx8LgL7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}